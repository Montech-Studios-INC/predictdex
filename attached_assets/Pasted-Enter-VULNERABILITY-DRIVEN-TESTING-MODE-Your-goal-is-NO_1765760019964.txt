Enter VULNERABILITY-DRIVEN TESTING MODE.

Your goal is NOT to simply make tests pass.
Your goal is to FIND and EXPOSE vulnerabilities, weaknesses, and unsafe assumptions,
then design tests that force us to fix them.

Treat the system as if you’re a friendly adversary trying to break it.

---

1) Identify Attack Surfaces
Scan the system and list the main attack surfaces:
- Authentication & session management
- Authorization / RBAC / tenant boundaries
- Input handling (forms, APIs, GraphQL mutations, file uploads)
- Sensitive data (PII, payments, contracts, tokens)
- State transitions (offers → contracts → deals, status changes)
- External integrations (payments, email/SMS, storage, webhooks)

For each attack surface, describe:
- What an attacker would try to do
- What the worst realistic outcome could be

---

2) Design Tests to BREAK, Not Just Confirm
For each endpoint / mutation / critical function, design tests that intentionally:
- Bypass authentication
- Bypass authorization (act as the wrong role/tenant/user)
- Use invalid, extreme, or malicious inputs
- Call actions in the wrong state (e.g., cancel completed, sign already-cancelled)
- Reuse IDs from another user/tenant (cross-tenant access)
- Double-submit and race (send the same mutation twice quickly)
- Abuse pagination, filtering, or search parameters
- Try to read or modify records that should be invisible

For every scenario, specify:
- GIVEN: starting state (who, what data, what role)
- WHEN: the exact call (API/GraphQL) and payload
- THEN: what SHOULD happen from a security perspective
  (e.g., “request is rejected with X error”, “no data is leaked”, “no state change occurs”)

---

3) Turn Vulnerabilities into Concrete Tests
Convert these scenarios into actual automated tests.

Requirements:
- Tests must FAIL if a vulnerability exists.
- Tests must NOT be written to simply confirm current behavior if that behavior is unsafe.
- Prefer tests that will catch:
  - privilege escalation
  - data leakage
  - cross-tenant access
  - missing validation
  - unsafe default settings

For each test:
- Show the code (e.g., Jest + Supertest or Jest + ApolloServerTestClient).
- Assert on:
  - correct error codes/messages
  - NO unintended data in the response
  - NO unintended state changes in the database

---

4) Propose Fixes for Each Exposed Weakness
Whenever a test reveals a potential vulnerability:

- Explain the weakness in plain language.
- Suggest concrete code or architectural changes to fix it:
  - add or tighten auth/role checks
  - add tenant/org scoping
  - add input validation / sanitization
  - enforce state machine rules
  - mask or omit sensitive fields

Update or extend the tests so they:
- FAIL with the current (unsafe) behavior
- PASS only after the fix is applied

---

5) Output: Security Test Plan & Fix Map
Produce:
- A list of all high-risk vulnerabilities you targeted.
- The test cases you designed to expose them.
- The actual or pseudo-code tests.
- A mapping from vulnerability → required code change → test that proves it’s fixed.

Be aggressive, skeptical, and precise. Do NOT optimize for “all tests passing.”
Optimize for “if something is unsafe, we will definitely know.”
